{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plit\n",
    "from skimage import io\n",
    "from skimage import color\n",
    "from skimage.feature import ORB\n",
    "from skimage.feature import match_descriptors\n",
    "from skimage.transform import ProjectiveTransform\n",
    "from skimage.measure import ransac\n",
    "from skimage.transform import SimilarityTransform\n",
    "from skimage.transform import warp\n",
    "from skimage.feature import match_descriptors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stitch():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def stitcher(self, img_arr_3):\n",
    "        keypoint_and_descriptors = []\n",
    "        matches=[]\n",
    "        robust_inliers=[]\n",
    "        warped_corners=[]\n",
    "        warped_images=[]\n",
    "        masked_images=[]\n",
    "        transforms=[]\n",
    "        #Loading Images and turning to rgb\n",
    "        y =lambda x: color.rgb2gray(io.imread(x))\n",
    "        x = list(map(y, img_arr_3))\n",
    "        r, c = x[0].shape[:2]\n",
    "        corners = np.array([[0, 0],[0, r], [c, 0], [c, r]])\n",
    "        #Creating ORB Instance\n",
    "        orb = ORB(n_keypoints=800, fast_threshold= 0.07)\n",
    "        #fetching ORB keypoints and descriptors\n",
    "        for img in x:\n",
    "            orb.detect_and_extract(img)\n",
    "            keypoint_and_descriptors.append((orb.keypoints, orb.descriptors))\n",
    "        #Matching the features in two pair of the images   \n",
    "        matches.append(match_descriptors(keypoint_and_descriptors[0][1], keypoint_and_descriptors[1][1], cross_check=True))\n",
    "        matches.append(match_descriptors(keypoint_and_descriptors[1][1], keypoint_and_descriptors[2][1], cross_check=True))\n",
    "        #We use the RANSAC algorithm to remove the extra features\n",
    "        for match in matches:\n",
    "            for iterr in range(len(keypoint_and_descriptors)):\n",
    "                src = keypoint_and_descriptors[iterr][match[:, 0]][:, ::-1]\n",
    "                dst = keypoint_and_descriptors[iterr+1][match[:, 1]][:, ::-1]\n",
    "                model_robust, inliers= ransac((src, dst), ProjectiveTransform, min_samples=4, residual_threshold=1, max_trials=300)\n",
    "                robust_inliers.append((model_robust, inliers))\n",
    "        #Warping the Images to stitch together\n",
    "        for _ in robust_inliers:\n",
    "            warped_corners.append(_[0](corners))\n",
    "        # Find the extents of both the reference image and the warped\n",
    "        # target image\n",
    "        all_corners = np.vstack((warped_corners[0], warped_corners[1], corners))\n",
    "        # The overally output shape will be max - min\n",
    "        corner_min = np.min(all_corners, axis=0)\n",
    "        corner_max = np.max(all_corners, axis=0)\n",
    "        output_shape = (corner_max - corner_min)\n",
    "        # Ensure integer shape with np.ceil and dtype conversion\n",
    "        output_shape = np.ceil(output_shape[::-1]).astype(int)\n",
    "        offset1 = SimilarityTransform(translation= -corner_min)\n",
    "        transform01 = (robust_inliers[0] + offset1).inverse\n",
    "        image1_warped = warp(x[0], transform01, order=3,output_shape=output_shape, cval=-1)\n",
    "        image1_mask = (image1_warped != -1)  # Mask == 1 inside image\n",
    "        image1_warped[~image1_mask] = 0      # Return background values to 0\n",
    "        image2_warped = warp(x[1], offset1.inverse, order=3, output_shape=output_shape, cval=-1)\n",
    "        image2_mask = (image2_warped != -1)  # Mask == 1 inside image\n",
    "        image2_warped[~image2_mask] = 0      # Return background values to 0\n",
    "        transform12 = (robust_inliers[1] + offset1).inverse\n",
    "        image3_warped = warp(x[2], transform12, order=3, output_shape=output_shape, cval=-1)\n",
    "        image3_mask = (image3_warped != -1)  # Mask == 1 inside image\n",
    "        image3_warped[~image3_mask] = 0\n",
    "        merged = (image1_warped + image2_warped + image3_warped)\n",
    "        overlap = (image1_mask* 1.0 + image2_mask + image3_mask)\n",
    "        normalized = merged / np.maximum(overlap, 1)\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-a55c87ab3286>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStitch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstitcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"1.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"2.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"3.jpg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-4cf1c68efee4>\u001b[0m in \u001b[0;36mstitcher\u001b[0;34m(self, img_arr_3)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0miterr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeypoint_and_descriptors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeypoint_and_descriptors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miterr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeypoint_and_descriptors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miterr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mmodel_robust\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minliers\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mransac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProjectiveTransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "instance = Stitch()\n",
    "result= instance.stitcher([\"1.jpg\",\"2.jpg\",\"3.jpg\"])\n",
    "fig, ax = plt.subplots(figsize=(15, 12))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
